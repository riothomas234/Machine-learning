{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! My name is Coniec! Give me a polynomial function and I will try to guess what the output is for an input that you provide - without explicitly calculating it!\n",
      "Enter your function here - remember, use strict python syntax: * for multiplication, / for division, ** for exponentiation, + for addition, -for subtraction and coefficients before the variable!-----:\n",
      "9*x**2-3*x\n",
      "Enter the upper and lower bounds for x values you would like to give me:\n",
      "30.0 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#intro\n",
    "\n",
    "print('Hi! My name is Coniec! Give me a polynomial function and I will try to guess what the output is for an input that you provide - without explicitly calculating it!')\n",
    "print('Enter your function here - remember, use strict python syntax: * for multiplication, / for division, ** for exponentiation, + for addition, -for subtraction and coefficients before the variable!-----:')\n",
    "function_input = input()\n",
    "print(function_input)\n",
    "#geet bounds\n",
    "print('Enter the upper and lower bounds for x values you would like to give me:')\n",
    "upper=float(input('Upper bound:'))\n",
    "lower=float(input('Lower bound:'))\n",
    "print(upper, lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training input values\n",
    "x_s = torch.arange(lower, upper, 0.11)\n",
    "\n",
    "\n",
    "#get function from user\n",
    "x_input = sp.symbols('x')\n",
    "function = sp.lambdify(x_input, function_input, 'numpy')\n",
    "\n",
    "# make training output values\n",
    "y_s = function(x_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose number of neurons in layer\n",
    "n_hidden_1 = 200\n",
    "\n",
    "#make parameters\n",
    "W1 = torch.randn(1, n_hidden_1)\n",
    "b1 = torch.randn(1, n_hidden_1)\n",
    "\n",
    "W2 = torch.randn(n_hidden_1,1)\n",
    "b2 = torch.randn(1,1)\n",
    "\n",
    "\n",
    "parameters = [W1, W2, b1, b2]\n",
    "\n",
    "\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "#repeat training 100000 times!\n",
    "\n",
    "\n",
    "#we calculate loss function of a batch of inputs to avoid overfitting...\n",
    "batch_size = 32\n",
    "\n",
    "#we normalise training data and train our multilayer perceptron on this. This stops our loss function from exploding!\n",
    "x_mean, x_std = x_s.mean(), x_s.std()\n",
    "y_mean, y_std = y_s.mean(), y_s.std()\n",
    "\n",
    "x_s_norm = (x_s - x_mean) / x_std\n",
    "y_s_norm = (y_s - y_mean) / y_std\n",
    "\n",
    "#define the loss function. This is a regression problem so we use the mean squared error loss function.\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/  40000 training rounds : loss = 43.2610\n",
      "  10000/  40000 training rounds : loss = 0.0008\n",
      "  20000/  40000 training rounds : loss = 0.0002\n",
      "  30000/  40000 training rounds : loss = 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#begin training\n",
    "max_steps = 40000\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    #generate batch indices, i.e. choose 32 random indices we can use to pick out values from our training set\n",
    "    ix = torch.randint(0,len(x_s), (batch_size,1) )\n",
    "    Xb, Yb = x_s_norm[ix], y_s_norm[ix]\n",
    "\n",
    "\n",
    "    #forward pass\n",
    "\n",
    "    #Linear layer\n",
    "    hpreact = Xb @ W1 + b1\n",
    "\n",
    "\n",
    "\n",
    "    #Non-linearity\n",
    "    h = torch.tanh(hpreact)\n",
    "\n",
    "    #2nd linear layer\n",
    "    output = h @ W2 + b2\n",
    "\n",
    "    #find loss\n",
    "    mse_loss = loss_fn(Yb, output)\n",
    "\n",
    "\n",
    "    #backward pass - we calculate gradient of loss function with respect to EVERY SINGLE PARAMETER.\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    mse_loss.backward()\n",
    "\n",
    "    #we update every parameter by a small step in the opposite direction to the partial derivative of the loss function with respect to this parameter.\n",
    "    #This changes each parameter as to minimise the loss function and make our neural network as accurate as possible. A classic step of the backpropagation algorithm.\n",
    "    lr = 0.01 if i <50000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "\n",
    "    #print our loss function value every now and then. Ideally it would keep decreasing, but here it fluctuate towards the end. We risk overfitting our neural net\n",
    "    #to our data. More time would have let me set up batch normalisation for each layer to combat this issue.\n",
    "    if i %10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d} training rounds : loss = {mse_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the y value is 1571.3685302734375\n",
      "The actual value is 1575.84 Was I close?, Type \"y\" if you want to input another x between 0.0 and 30.0 or any other key to stop \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sample from model\n",
    "flag=True\n",
    "while flag==True:\n",
    "    x_sample = float(input(f'enter an x value between {lower} and {upper} to enter into your function {function_input}: '))\n",
    "    #our sample is normalised\n",
    "    x_sample_norm = (x_sample-x_mean)/x_std\n",
    "    x_sample_norm = torch.tensor([[x_sample_norm]])\n",
    "    y_sample_norm = torch.tanh(x_sample_norm @ W1 +b1) @ W2 + b2\n",
    "    #the network output is denormalised before being displayed.\n",
    "    y_sample =  y_sample_norm*y_std+y_mean\n",
    "    print('I think the y value is', y_sample.item())\n",
    "    print(f'The actual value is {function(x_sample)} Was I close?, Type \"y\" if you want to input another x between {lower} and {upper} or any other key to stop ')\n",
    "    cont = input()\n",
    "    if cont=='y':\n",
    "        pass\n",
    "    else:\n",
    "        flag=False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
