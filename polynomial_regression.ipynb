{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/local/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n",
      "    return await f(*args, **kwargs)\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/21/27_gy57n0s5d93lj2z_tbx580000gn/T/ipykernel_2532/3997714732.py\", line 1, in <module>\n",
      "    import torch.nn\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/rthomas/Documents/GitHub/Machine-learning/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch.nn\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! My name is Coniec! Give me a polynomial function and I will try to guess what the output is for an input that you provide - without explicitly calculating it!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#intro\n",
    "\n",
    "print('Hi! My name is Coniec! Give me a polynomial function and I will try to guess what the output is for an input that you provide - without explicitly calculating it!')\n",
    "function_input = input('Enter your function here - remember, use strict python syntax: * for multiplication, / for division, ** for exponentiation, + for addition, -for subtraction and coefficients before the variable!-----:')\n",
    "\n",
    "#geet bounds\n",
    "print('Enter the upper and lower bounds for x values you would like to give me:')\n",
    "upper=float(input('Upper bound:'))\n",
    "lower=float(input('Lower bound:'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training input values\n",
    "x_s = torch.arange(lower, upper, 0.11)\n",
    "\n",
    "\n",
    "#get function from user\n",
    "x_input = sp.symbols('x')\n",
    "function = sp.lambdify(x_input, function_input, 'numpy')\n",
    "\n",
    "# make training output values\n",
    "y_s = function(x_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose number of neurons in layer\n",
    "n_hidden_1 = 200\n",
    "\n",
    "#make parameters\n",
    "W1 = torch.randn(1, n_hidden_1)\n",
    "b1 = torch.randn(1, n_hidden_1)\n",
    "\n",
    "W2 = torch.randn(n_hidden_1,1)\n",
    "b2 = torch.randn(1,1)\n",
    "\n",
    "\n",
    "parameters = [W1, W2, b1, b2]\n",
    "\n",
    "\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "#repeat training 100000 times!\n",
    "max_steps = 100000\n",
    "\n",
    "#we calculate loss function of a batch of inputs to avoid overfitting...\n",
    "batch_size = 32\n",
    "\n",
    "#we normalise training data and train our multilayer perceptron on this. This stops our loss function from exploding!\n",
    "x_mean, x_std = x_s.mean(), x_s.std()\n",
    "y_mean, y_std = y_s.mean(), y_s.std()\n",
    "\n",
    "x_s_norm = (x_s - x_mean) / x_std\n",
    "y_s_norm = (y_s - y_mean) / y_std\n",
    "\n",
    "#define the loss function. This is a regression problem so we use the mean squared error loss function.\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 100000 training rounds : loss = 436.3403\n",
      "  10000/ 100000 training rounds : loss = 0.0002\n",
      "  20000/ 100000 training rounds : loss = 0.0001\n",
      "  30000/ 100000 training rounds : loss = 0.0000\n",
      "  40000/ 100000 training rounds : loss = 0.0000\n",
      "  50000/ 100000 training rounds : loss = 0.0000\n",
      "  60000/ 100000 training rounds : loss = 0.0000\n",
      "  70000/ 100000 training rounds : loss = 0.0000\n",
      "  80000/ 100000 training rounds : loss = 0.0000\n",
      "  90000/ 100000 training rounds : loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#begin training\n",
    "for i in range(max_steps):\n",
    "\n",
    "    #generate batch indices, i.e. choose 32 random indices we can use to pick out values from our training set\n",
    "    ix = torch.randint(0,len(x_s), (batch_size,1) )\n",
    "    Xb, Yb = x_s_norm[ix], y_s_norm[ix]\n",
    "\n",
    "\n",
    "    #forward pass\n",
    "\n",
    "    #Linear layer\n",
    "    hpreact = Xb @ W1 + b1\n",
    "\n",
    "\n",
    "\n",
    "    #Non-linearity\n",
    "    h = torch.tanh(hpreact)\n",
    "\n",
    "    #2nd linear layer\n",
    "    output = h @ W2 + b2\n",
    "\n",
    "    #find loss\n",
    "    mse_loss = loss_fn(Yb, output)\n",
    "\n",
    "\n",
    "    #backward pass - we calculate gradient of loss function with respect to EVERY SINGLE PARAMETER.\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    mse_loss.backward()\n",
    "\n",
    "    #we update every parameter by a small step in the opposite direction to the partial derivative of the loss function with respect to this parameter.\n",
    "    #This changes each parameter as to minimise the loss function and make our neural network as accurate as possible. A classic step of the backpropagation algorithm.\n",
    "    lr = 0.01 if i <50000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "\n",
    "    #print our loss function value every now and then. Ideally it would keep decreasing, but here it fluctuate towards the end. We risk overfitting our neural net\n",
    "    #to our data. More time would have let me set up batch normalisation for each layer to combat this issue.\n",
    "    if i %10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d} training rounds : loss = {mse_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter an x value between 0.0 and 100.0 to enter into your function 9*x**2-3*x: \n",
      "I think the y value is 77451.421875\n",
      "The actual value is 77562.0 Was I close?, Type \"y\" if you want to input another x between 0.0 and 100.0 or any other key to stop \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sample from model\n",
    "flag=True\n",
    "while flag==True:\n",
    "    x_sample = float(input(f'enter an x value between {lower} and {upper} to enter into your function {function_input}: '))\n",
    "    #our sample is normalised\n",
    "    x_sample_norm = (x_sample-x_mean)/x_std\n",
    "    x_sample_norm = torch.tensor([[x_sample_norm]])\n",
    "    y_sample_norm = torch.tanh(x_sample_norm @ W1 +b1) @ W2 + b2\n",
    "    #the network output is denormalised before being displayed.\n",
    "    y_sample =  y_sample_norm*y_std+y_mean\n",
    "    print('I think the y value is', y_sample.item())\n",
    "    cont = input(f'The actual value is {function(x_sample)} Was I close?, Type \"y\" if you want to input another x between {lower} and {upper} or any other key to stop ')\n",
    "    if cont=='y':\n",
    "        pass\n",
    "    else:\n",
    "        flag=False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
